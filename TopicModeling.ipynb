{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70c09456-5782-4c1c-955a-66b014716563",
   "metadata": {},
   "source": [
    "***Topic Modeling Using Latent Dirichlet Allocation (LDA)***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f09e4a-048c-4e95-869f-8449bf312cfe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### installing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3040e616-2a9e-436c-8cd4-76679f34d914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (8.1.7)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from ipywidgets) (8.25.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: decorator in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f9f31a2-d4f8-4fee-ad25-61954de42170",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub[hf_xet] in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (0.31.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from huggingface_hub[hf_xet]) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from huggingface_hub[hf_xet]) (2025.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from huggingface_hub[hf_xet]) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from huggingface_hub[hf_xet]) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from huggingface_hub[hf_xet]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.11.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.1 (from huggingface_hub[hf_xet])\n",
      "  Downloading hf_xet-1.1.2-cp37-abi3-win_amd64.whl.metadata (883 bytes)\n",
      "Requirement already satisfied: colorama in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub[hf_xet]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2025.1.31)\n",
      "Downloading hf_xet-1.1.2-cp37-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------- ----------------- 1.6/2.7 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 8.8 MB/s eta 0:00:00\n",
      "Installing collected packages: hf-xet\n",
      "Successfully installed hf-xet-1.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install huggingface_hub[hf_xet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b60ff2e-653e-417f-aeaf-5e30ce5864a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bertopic\n",
      "  Using cached bertopic-0.17.0-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting hdbscan>=0.8.29 (from bertopic)\n",
      "  Using cached hdbscan-0.8.40-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from bertopic) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.1.5 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from bertopic) (2.2.2)\n",
      "Requirement already satisfied: plotly>=4.7.0 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from bertopic) (6.1.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from bertopic) (1.5.2)\n",
      "Collecting sentence-transformers>=0.4.1 (from bertopic)\n",
      "  Using cached sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from bertopic) (4.67.1)\n",
      "Collecting umap-learn>=0.5.0 (from bertopic)\n",
      "  Using cached umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from hdbscan>=0.8.29->bertopic) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from pandas>=1.1.5->bertopic) (2023.3)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from plotly>=4.7.0->bertopic) (1.40.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from plotly>=4.7.0->bertopic) (24.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from scikit-learn>=1.0->bertopic) (3.5.0)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers>=0.4.1->bertopic)\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (2.7.0)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers>=0.4.1->bertopic)\n",
      "  Using cached huggingface_hub-0.31.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from sentence-transformers>=0.4.1->bertopic) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from tqdm>=4.41.1->bertopic) (0.4.6)\n",
      "Collecting numba>=0.51.2 (from umap-learn>=0.5.0->bertopic)\n",
      "  Using cached numba-0.61.2-cp312-cp312-win_amd64.whl.metadata (2.9 kB)\n",
      "Collecting pynndescent>=0.5 (from umap-learn>=0.5.0->bertopic)\n",
      "  Using cached pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2025.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.32.3)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.44.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.16.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (72.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2024.11.6)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.5.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2025.1.31)\n",
      "Using cached bertopic-0.17.0-py3-none-any.whl (150 kB)\n",
      "Using cached hdbscan-0.8.40-cp312-cp312-win_amd64.whl (726 kB)\n",
      "Using cached sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Using cached umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
      "Using cached huggingface_hub-0.31.2-py3-none-any.whl (484 kB)\n",
      "Using cached numba-0.61.2-cp312-cp312-win_amd64.whl (2.8 MB)\n",
      "Using cached pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Installing collected packages: numba, huggingface-hub, tokenizers, pynndescent, hdbscan, umap-learn, transformers, sentence-transformers, bertopic\n",
      "Successfully installed bertopic-0.17.0 hdbscan-0.8.40 huggingface-hub-0.31.2 numba-0.61.2 pynndescent-0.5.13 sentence-transformers-4.1.0 tokenizers-0.21.1 transformers-4.51.3 umap-learn-0.5.7\n"
     ]
    }
   ],
   "source": [
    "!pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70735262-4c6d-4b6b-b33d-488217a9eb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp312-cp312-win_amd64.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\crist\\anaconda3\\envs\\sds\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Downloading gensim-4.3.3-cp312-cp312-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 3.7/24.0 MB 19.8 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 6.0/24.0 MB 14.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 10.2/24.0 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 13.6/24.0 MB 16.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 17.3/24.0 MB 17.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.3/24.0 MB 17.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.9/24.0 MB 17.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.0/24.0 MB 15.8 MB/s eta 0:00:00\n",
      "Downloading scipy-1.13.1-cp312-cp312-win_amd64.whl (45.9 MB)\n",
      "   ---------------------------------------- 0.0/45.9 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 3.9/45.9 MB 18.1 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 8.1/45.9 MB 20.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 13.9/45.9 MB 22.4 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 18.4/45.9 MB 22.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 22.3/45.9 MB 21.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 28.3/45.9 MB 22.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 33.6/45.9 MB 22.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 37.5/45.9 MB 22.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 41.4/45.9 MB 22.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  45.9/45.9 MB 22.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  45.9/45.9 MB 22.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 45.9/45.9 MB 18.5 MB/s eta 0:00:00\n",
      "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Installing collected packages: smart-open, scipy, gensim\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.14.1\n",
      "    Uninstalling scipy-1.14.1:\n",
      "      Successfully uninstalled scipy-1.14.1\n",
      "Successfully installed gensim-4.3.3 scipy-1.13.1 smart-open-7.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\crist\\AppData\\Local\\Temp\\pip-uninstall-5lao3pmv'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d22d8684-a149-4f14-9046-ec0f3370c70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\crist\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\crist\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\crist\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\crist\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\crist\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\crist\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')       # For word_tokenize\n",
    "nltk.download('wordnet')     # For lemmatizer\n",
    "nltk.download('stopwords')   # For stopwords\n",
    "nltk.download('omw-1.4')     # For lemmatizer support\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adec50c-e556-4ed7-bd07-e2d655835597",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Sample data 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0ceda84-1f10-4c31-8ad8-d1338a5cb9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.matutils import corpus2csc\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c31c6c69-fd6f-4459-886d-0663400e01fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('sample.csv')\n",
    "sample_data=df['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f006423-cf21-4773-a854-ce911d0b60a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [found, subreddit, ,, hell, ., make, sad, angr...\n",
       "1     [get, sub, gotten, lot, bigger, past, day, ’, ...\n",
       "2     [lesbian, lucky, enough, live, way, year, ., a...\n",
       "3     [saw, thought, great, ., thought, good, place,...\n",
       "4     [tired, fed, seeing, men, uncle, culture, fyp,...\n",
       "5     [4b, woman, always, considered, “, conventiona...\n",
       "6     [men, fetishize, everything, ,, incest, ,, rap...\n",
       "7     [certain, supplement, ,, especially, high, dos...\n",
       "8     [hey, beautiful, soul, 💙, made, detox, social,...\n",
       "9     [context, (, 18f, ), religious, conservative, ...\n",
       "10                                              [found]\n",
       "11    [,, sure, might, think, reaching, ,, wanted, s...\n",
       "12    [dug, old, jewelry, making, kit, craft, room, ...\n",
       "13    [decided, make, sub, woman, post, experience, ...\n",
       "14    [*, apology, ,, *, *, mood, ., need, week, soc...\n",
       "15    [news/information, resurfaced, regarding, 2006...\n",
       "16    [good, book, recs, support, founding, principl...\n",
       "17    [woman, living, uk, ,, stand, ., heart, breaki...\n",
       "18    [’, men, begin, ,, going, bothered, woman, ’, ...\n",
       "19    [title, :, 45, %, woman, estimated, single, ch...\n",
       "20    [want, something, subtle, slightly, threatenin...\n",
       "21    [hi, ,, (, 18f, ), currently, tube, video, bum...\n",
       "22    ['m, sad, honestly, ., feel, like, 've, always...\n",
       "23    [y'all, know, 4b, movement, need, ?, grassroot...\n",
       "24    [welcome, woman, discovering, considering, ado...\n",
       "25    [bit, light, hearted, story, ,, still, frustra...\n",
       "26    [disclaimer, may, relationship, ,, given, curr...\n",
       "27          [🕯, future, dark, stand, together, ., 🕯, 🐝]\n",
       "28    [“, search, male, family, friend, phone, ,, co...\n",
       "29    [http, :, //www.reddit.com/r/national_strike/c...\n",
       "30    [possible, ,, also, ,, obviously, ,, discretio...\n",
       "31    [hello, !, like, everyone, upset, election, re...\n",
       "32    [spent, year, trying, dissect, sexuality, figu...\n",
       "33    [think, 'm, trans, masc, want, start, 'm, real...\n",
       "34    ['s, ., 's, post, ., know, want, flower, 's, d...\n",
       "35    [point, blank, ., live, age, everything, regul...\n",
       "36                                       [fuck, men, .]\n",
       "37    [way, practicing, movement, (, unknowingly, ),...\n",
       "38    [humanity, sinking, due, ultra-consumist, self...\n",
       "39    [love, quote, notre, dame, maryland, u, philos...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatization and removing stopwords\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "#function to lemmatize and remove stopwords from the text data\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    words = word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return words\n",
    "\n",
    "\n",
    "#applying the function to the dataset\n",
    "sample_data = sample_data.apply(preprocess)\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd8223c-bba9-4e23-967b-a94a793a4b6a",
   "metadata": {},
   "source": [
    "The code below processes the data to remove words that appear in fewer than 5 documents and those that appear in more than 50% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c3167c22-5812-4e6c-aa75-3d41f2d6c527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "Words: [':', '?', \"'s\", 'men', 'woman', 'movement', 'think', 'know', 'http', \"n't\"]\n",
      "Topic: 1\n",
      "Words: ['’', 'want', 'woman', 'going', 'know', 'love', '(', ')', 'even', 'without']\n",
      "Topic: 2\n",
      "Words: ['’', 'woman', 'men', '“', '”', 'life', 'know', 'time', 'still', '4b']\n",
      "Topic: 3\n",
      "Words: ['people', \"'s\", 'get', 'right', 'made', 'would', \"n't\", '4b', 'support', 'also']\n",
      "Topic: 4\n",
      "Words: ['man', 'men', 'woman', 'want', 'also', '(', ')', 'make', 'think', \"n't\"]\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary from the preprocessed data\n",
    "dictionary = Dictionary(sample_data)\n",
    "\n",
    "# Filter out words that appear in fewer than 5 documents or more than 50% of the documents\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in data]\n",
    "\n",
    "# Train the LDA model\n",
    "num_topics = 5\n",
    "ldamodel = LdaModel(bow_corpus, num_topics=num_topics, id2word=dictionary, passes=20, alpha='auto', eta='auto')\n",
    "\n",
    "# Get the topics\n",
    "topics = ldamodel.show_topics(num_topics=num_topics, num_words=10, log=False, formatted=False)\n",
    "\n",
    "# Print the topics\n",
    "for topic_id, topic in topics:\n",
    "    print(\"Topic: {}\".format(topic_id))\n",
    "    print(\"Words: {}\".format([word for word, _ in topic]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91af48c8-8919-4b16-86d0-0dbaa363136c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Full data 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91f95c10-1ba7-4394-bad3-cbbdbfb27351",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.read_csv('merged.csv')\n",
    "cleaned=df2[~df2['content'].isin(['[removed]', '[deleted]'])]\n",
    "data2=cleaned['content'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "519bf331-2a23-422b-ac6f-319f5ab24a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        I cant be truly 4b because im in a relationshi...\n",
       "1                                           I'm a lesbian.\n",
       "2                          We need more people like you ❤️\n",
       "3        I just ended things with a FWB who didn’t valu...\n",
       "4                                                        🤏\n",
       "                               ...                        \n",
       "43639                                                  nan\n",
       "43641                     They can’t even hide it anymore.\n",
       "43642    I've been sober since 2016 and I have a lot of...\n",
       "43643                                                  nan\n",
       "43644    I'm so sick of stories like this yet another h...\n",
       "Name: content, Length: 36839, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "259e487f-198b-459f-b1b7-0b735d2830ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Helper to convert POS tag to WordNet POS\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun\n",
    "\n",
    "# Preprocessing function that returns a list\n",
    "def preprocess(text):\n",
    "    if pd.isnull(text):\n",
    "        return []\n",
    "\n",
    "    # Clean text\n",
    "    text = re.sub(r'@\\w+', '', text)         # Remove mentions\n",
    "    text = re.sub(r'\\d+', '', text)          # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Normalize whitespace\n",
    "\n",
    "    # Tokenize and lower\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # POS tagging\n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    # Lemmatize and remove stopwords/punctuation\n",
    "    lemmatized = [\n",
    "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "        for word, pos in pos_tags\n",
    "        if word.isalpha() and word not in stop_words\n",
    "    ]\n",
    "\n",
    "    return lemmatized  # Return as a list\n",
    "\n",
    "# Apply to your dataset\n",
    "data3 = data2.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fdd79ac-d083-406e-8678-b6f9723cc0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3_cleaned = data3.dropna()\n",
    "data3_cleaned = data3_cleaned[data3_cleaned.apply(lambda x: isinstance(x, list) and any(pd.notna(i) and i != '' for i in x))]\n",
    "# Reset index\n",
    "data = data3_cleaned.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8e7c62e-2f69-48e2-b757-9469e9fcb34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [cant, truly, b, im, relationship, think, ive,...\n",
       "1                                                [lesbian]\n",
       "2                                     [need, people, like]\n",
       "3        [end, thing, fwb, value, respect, tired, feel,...\n",
       "4        [honestly, still, wide, variety, option, depen...\n",
       "                               ...                        \n",
       "36261                                                [nan]\n",
       "36262                                [even, hide, anymore]\n",
       "36263    [sober, since, lot, helpful, information, scie...\n",
       "36264                                                [nan]\n",
       "36265    [sick, story, like, yet, another, husband, kil...\n",
       "Name: content, Length: 36266, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb65534d-7ffe-4011-a84c-f84dd6b9613e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "Words: ['post', 'comment', 'share', 'thank', 'read', 'please', 'sub', 'write', 'book', 'add']\n",
      "Topic: 1\n",
      "Words: ['woman', 'b', 'movement', 'support', 'community', 'society', 'patriarchy', 'feminist', 'group', 'create']\n",
      "Topic: 2\n",
      "Words: ['na', 'block', 'gon', 'character', 'wan', 'revolution', 'trade', 'yep', 'privilege', 'wild']\n",
      "Topic: 3\n",
      "Words: ['buy', 'money', 'pay', 'class', 'business', 'product', 'art', 'car', 'sell', 'door']\n",
      "Topic: 4\n",
      "Words: ['rape', 'country', 'violence', 'state', 'birth', 'report', 'kill', 'domestic', 'law', 'risk']\n",
      "Topic: 5\n",
      "Words: ['child', 'kid', 'family', 'young', 'marry', 'girl', 'mother', 'husband', 'marriage', 'mom']\n",
      "Topic: 6\n",
      "Words: ['time', 'life', 'year', 'work', 'go', 'live', 'friend', 'find', 'start', 'day']\n",
      "Topic: 7\n",
      "Words: ['http', 'female', 'watch', 'video', 'social', 'medium', 'game', 'follow', 'online', 'content']\n",
      "Topic: 8\n",
      "Words: ['woman', 'men', 'get', 'want', 'even', 'would', 'one', 'know', 'u', 'need']\n",
      "Topic: 9\n",
      "Words: ['like', 'think', 'feel', 'say', 'make', 'people', 'really', 'love', 'look', 'thing']\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary from the preprocessed data\n",
    "dictionary = Dictionary(data)\n",
    "\n",
    "# Filter out words that appear in fewer than 5 documents or more than 50% of the documents\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in data]\n",
    "\n",
    "# Train the LDA model\n",
    "num_topics = 10\n",
    "ldamodel = LdaModel(bow_corpus, num_topics=num_topics, id2word=dictionary, passes=20, alpha='auto', eta='auto')\n",
    "\n",
    "# Get the topics\n",
    "topics = ldamodel.show_topics(num_topics=num_topics, num_words=10, log=False, formatted=False)\n",
    "\n",
    "#Store and Print the topics\n",
    "with open(\"LDA_topics.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for topic_id, topic in topics:\n",
    "        print(\"Topic: {}\".format(topic_id))\n",
    "        print(\"Words: {}\".format([word for word, _ in topic]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cbcdd5-66f1-4cb4-80f4-6c2282464eb6",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e529a6-76d2-4790-be7e-fac085e958fb",
   "metadata": {},
   "source": [
    "## Full content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3770b6f6-4856-4b99-9610-1a3c7f99b6ff",
   "metadata": {},
   "source": [
    "documentation: https://colab.research.google.com/drive/15zIjrFqZsJ-bbTueDOaXhhVfFIo-i22B?usp=sharing#scrollTo=FX_Ngb3PpRFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ac665e8-9865-4b0b-a553-4774f95bc7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07f82a78-b143-443f-88db-84710af5dfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 12:47:21,008 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7929ec4ca046c1a9488290b0deb7ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 12:58:29,392 - BERTopic - Embedding - Completed ✓\n",
      "2025-05-20 12:58:29,392 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-05-20 12:59:15,800 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-05-20 12:59:15,818 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-05-20 13:14:23,762 - BERTopic - Cluster - Completed ✓\n",
      "2025-05-20 13:14:23,808 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-05-20 13:14:26,319 - BERTopic - Representation - Completed ✓\n"
     ]
    }
   ],
   "source": [
    "df2=pd.read_csv('merged.csv')\n",
    "cleaned=df2[~df2['content'].isin(['[removed]', '[deleted]'])]\n",
    "data2=cleaned['content'].astype(str)\n",
    "topic_model = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True)\n",
    "topics, probs = topic_model.fit_transform(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01d07067-239a-41f6-ba80-a4aadf2b134b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>18004</td>\n",
       "      <td>-1_to_and_they_the</td>\n",
       "      <td>[to, and, they, the, of, that, it, in, you, for]</td>\n",
       "      <td>[&gt; You can ask a man nicely to do something an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>472</td>\n",
       "      <td>0_4b_movement_feminism_decentering</td>\n",
       "      <td>[4b, movement, feminism, decentering, men, abo...</td>\n",
       "      <td>[Sorry, but 4b isn't a sort of revenge to make...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>358</td>\n",
       "      <td>1_korean_korea_south_movement</td>\n",
       "      <td>[korean, korea, south, movement, 4b, feminists...</td>\n",
       "      <td>[and they say there is no 4b movement in Korea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>334</td>\n",
       "      <td>2_dad_she_mom_her</td>\n",
       "      <td>[dad, she, mom, her, mother, father, my, he, h...</td>\n",
       "      <td>[My mom died of cancer in August this year. I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>292</td>\n",
       "      <td>3_4b_movement_post_6b</td>\n",
       "      <td>[4b, movement, post, 6b, 7b, relevant, join, i...</td>\n",
       "      <td>[4B for life! 🙌❤️, 4B, 4B!]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>290</td>\n",
       "      <td>4_nan_macaron_joseph_disastrous</td>\n",
       "      <td>[nan, macaron, joseph, disastrous, himself, te...</td>\n",
       "      <td>[nan, nan, nan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>252</td>\n",
       "      <td>5_christmas_holidays_thanksgiving_cook</td>\n",
       "      <td>[christmas, holidays, thanksgiving, cook, cook...</td>\n",
       "      <td>[Nope.  I'm spending a quiet Christmas by myse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>246</td>\n",
       "      <td>6_hugs_tk_oo_vcels</td>\n",
       "      <td>[hugs, tk, oo, vcels, mah, darwin, omega, exho...</td>\n",
       "      <td>[Hugs💜, Hugs💜, Hugs💜]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>243</td>\n",
       "      <td>7_trans_cis_transphobia_transphobic</td>\n",
       "      <td>[trans, cis, transphobia, transphobic, terf, t...</td>\n",
       "      <td>[Hot take, but I'd leave out trans men.\\n\\nTra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>203</td>\n",
       "      <td>8_marriage_divorce_married_arranged</td>\n",
       "      <td>[marriage, divorce, married, arranged, marry, ...</td>\n",
       "      <td>[I HATE happy marriage. What now?, Did she say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>192</td>\n",
       "      <td>9_religion_religions_church_religious</td>\n",
       "      <td>[religion, religions, church, religious, god, ...</td>\n",
       "      <td>[Women should start their own religion, free o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>191</td>\n",
       "      <td>10_song_songs_playlist_music</td>\n",
       "      <td>[song, songs, playlist, music, listen, album, ...</td>\n",
       "      <td>[Maybe Studio Killers. I haven't listened much...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>190</td>\n",
       "      <td>11_friends_friendships_friend_friendship</td>\n",
       "      <td>[friends, friendships, friend, friendship, pla...</td>\n",
       "      <td>[I lost so many male friends because they were...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>183</td>\n",
       "      <td>12_proud_thank_hugs_sending</td>\n",
       "      <td>[proud, thank, hugs, sending, strong, you, hop...</td>\n",
       "      <td>[Proud of you., I'm so proud of you!!!, Thank ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>174</td>\n",
       "      <td>13_makeup_hair_wear_skin</td>\n",
       "      <td>[makeup, hair, wear, skin, beauty, skincare, w...</td>\n",
       "      <td>[As someone who enjoys makeup, I don’t care ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14</td>\n",
       "      <td>164</td>\n",
       "      <td>14_rape_rapists_rapist_raped</td>\n",
       "      <td>[rape, rapists, rapist, raped, rapes, marital,...</td>\n",
       "      <td>[&gt;  A woman supporting rapists is the same as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15</td>\n",
       "      <td>162</td>\n",
       "      <td>15_liberal_conservative_maga_conservatives</td>\n",
       "      <td>[liberal, conservative, maga, conservatives, w...</td>\n",
       "      <td>[&gt;Oh but I thought maga men DIDN’T want to dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16</td>\n",
       "      <td>162</td>\n",
       "      <td>16_hair_shaving_shave_shaved</td>\n",
       "      <td>[hair, shaving, shave, shaved, head, heads, le...</td>\n",
       "      <td>[Shaving ones head does not make them ugly., I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17</td>\n",
       "      <td>161</td>\n",
       "      <td>17_misandry_misogyny_misandrist_misogynists</td>\n",
       "      <td>[misandry, misogyny, misandrist, misogynists, ...</td>\n",
       "      <td>[Misandry doesn't exist, \"misandry\" lol, So mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18</td>\n",
       "      <td>153</td>\n",
       "      <td>18_join_id_please_hi</td>\n",
       "      <td>[join, id, please, hi, joined, sign, love, joi...</td>\n",
       "      <td>[I’d like to join!, I’d like to join, I’d like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19</td>\n",
       "      <td>153</td>\n",
       "      <td>19_ally_allies_allyship_an</td>\n",
       "      <td>[ally, allies, allyship, an, 4b, here, partici...</td>\n",
       "      <td>[It only shows why they are really \"allies\", n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20</td>\n",
       "      <td>145</td>\n",
       "      <td>20_loneliness_epidemic_lonely_male</td>\n",
       "      <td>[loneliness, epidemic, lonely, male, men, them...</td>\n",
       "      <td>[“Male loneliness” They do it to themselves, “...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>21</td>\n",
       "      <td>143</td>\n",
       "      <td>21_porn_bdsm_watching_pornography</td>\n",
       "      <td>[porn, bdsm, watching, pornography, consent, s...</td>\n",
       "      <td>[nope. all porn is rape, all porn is misogynis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>22</td>\n",
       "      <td>133</td>\n",
       "      <td>22_sub_subs_reddit_banned</td>\n",
       "      <td>[sub, subs, reddit, banned, subreddit, mods, s...</td>\n",
       "      <td>[No, it does not need to be shared in women’s ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>23</td>\n",
       "      <td>130</td>\n",
       "      <td>23_gay_straight_lesbians_allies</td>\n",
       "      <td>[gay, straight, lesbians, allies, homophobic, ...</td>\n",
       "      <td>[lol. Obviously. Recentering men. For example,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>24</td>\n",
       "      <td>129</td>\n",
       "      <td>24_love_this_omg_lmao</td>\n",
       "      <td>[love, this, omg, lmao, brilliant, mindfck, ba...</td>\n",
       "      <td>[I love this! 🐝🐝🐝🐝, I love this!, I LOVE THIS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>25</td>\n",
       "      <td>128</td>\n",
       "      <td>25_fat_body_emmanuel_breasts</td>\n",
       "      <td>[fat, body, emmanuel, breasts, breast, height,...</td>\n",
       "      <td>[Males fetishize fat. They love fat in breasts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>26</td>\n",
       "      <td>123</td>\n",
       "      <td>26_dming_you_yup_yes</td>\n",
       "      <td>[dming, you, yup, yes, course, both, sure, me,...</td>\n",
       "      <td>[DMing you!, DMing you!, DMing you!]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>27</td>\n",
       "      <td>120</td>\n",
       "      <td>27_apps_dating_app_deleting</td>\n",
       "      <td>[apps, dating, app, deleting, profile, delete,...</td>\n",
       "      <td>[Dating apps are for men, and women are the pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>28</td>\n",
       "      <td>120</td>\n",
       "      <td>28_ignore_attention_them_energy</td>\n",
       "      <td>[ignore, attention, them, energy, engage, wast...</td>\n",
       "      <td>[Ignore. Ignore. Ignore. This is the way., Ign...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>29</td>\n",
       "      <td>119</td>\n",
       "      <td>29_spray_pepper_carry_knife</td>\n",
       "      <td>[spray, pepper, carry, knife, weapon, carrying...</td>\n",
       "      <td>[Pepper gel is what the cops use.  It's suppos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>30</td>\n",
       "      <td>114</td>\n",
       "      <td>30_smile_stare_staring_face</td>\n",
       "      <td>[smile, stare, staring, face, eye, smiling, lo...</td>\n",
       "      <td>[Trump has a Grinch smile. They smile exactly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>31</td>\n",
       "      <td>113</td>\n",
       "      <td>31_men_all_nutshell_bootstraps</td>\n",
       "      <td>[men, all, nutshell, bootstraps, pedos, yes, n...</td>\n",
       "      <td>[All men., All men. , Men have to wear the boo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>32</td>\n",
       "      <td>113</td>\n",
       "      <td>32_him_reconcile_he_explanation</td>\n",
       "      <td>[him, reconcile, he, explanation, leave, you, ...</td>\n",
       "      <td>[Please do not reconcile  with him, he has alr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>33</td>\n",
       "      <td>112</td>\n",
       "      <td>33_gun_guns_defense_handgun</td>\n",
       "      <td>[gun, guns, defense, handgun, firearm, carry, ...</td>\n",
       "      <td>[Guns, I want a gun like that, Women are more ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>34</td>\n",
       "      <td>112</td>\n",
       "      <td>34_custody_motherhood_children_kids</td>\n",
       "      <td>[custody, motherhood, children, kids, mothers,...</td>\n",
       "      <td>[&gt;*\"Abusive men often want and ask for full cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>35</td>\n",
       "      <td>110</td>\n",
       "      <td>35_therapist_therapists_therapy_statistics</td>\n",
       "      <td>[therapist, therapists, therapy, statistics, i...</td>\n",
       "      <td>[No, no, no, no, no. It’s not your job to conv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>36</td>\n",
       "      <td>108</td>\n",
       "      <td>36_white_black_color_racism</td>\n",
       "      <td>[white, black, color, racism, racist, indigeno...</td>\n",
       "      <td>[She's making points against the Dogma of Whit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>37</td>\n",
       "      <td>106</td>\n",
       "      <td>37_movement_separatism_support_separatists</td>\n",
       "      <td>[movement, separatism, support, separatists, f...</td>\n",
       "      <td>[I’m all for 4B women and non-4B women working...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>38</td>\n",
       "      <td>105</td>\n",
       "      <td>38_cat_cats_lady_feral</td>\n",
       "      <td>[cat, cats, lady, feral, dogs, black, kitties,...</td>\n",
       "      <td>[I'm a cat lady! Cats are better companions th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>39</td>\n",
       "      <td>105</td>\n",
       "      <td>39_ladies_thank_love_women</td>\n",
       "      <td>[ladies, thank, love, women, beautiful, wink, ...</td>\n",
       "      <td>[I love this, thank you so much for the suppor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>40</td>\n",
       "      <td>103</td>\n",
       "      <td>40_they_stupid_us_care</td>\n",
       "      <td>[they, stupid, us, care, dont, were, deserve, ...</td>\n",
       "      <td>[Some are stupid enough to actually do that!, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>41</td>\n",
       "      <td>102</td>\n",
       "      <td>41_dress_wear_dressing_makeup</td>\n",
       "      <td>[dress, wear, dressing, makeup, wearing, style...</td>\n",
       "      <td>[Dress for yourself, no-one else, Weird. I dre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>42</td>\n",
       "      <td>98</td>\n",
       "      <td>42_he_store_smoke_uber</td>\n",
       "      <td>[he, store, smoke, uber, report, bag, shop, hi...</td>\n",
       "      <td>[I'm nice to everyone I feel familiar with, es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>43</td>\n",
       "      <td>98</td>\n",
       "      <td>43_violence_violent_crime_victims</td>\n",
       "      <td>[violence, violent, crime, victims, perpetrato...</td>\n",
       "      <td>[I recently came across the Laken Riley murder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>44</td>\n",
       "      <td>97</td>\n",
       "      <td>44_gym_weights_gyms_lifting</td>\n",
       "      <td>[gym, weights, gyms, lifting, membership, lift...</td>\n",
       "      <td>[I do private gym or lift weights at home or r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>45</td>\n",
       "      <td>96</td>\n",
       "      <td>45_state_blue_texas_moving</td>\n",
       "      <td>[state, blue, texas, moving, move, florida, lo...</td>\n",
       "      <td>[ I plan to save as much as possible and leave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>46</td>\n",
       "      <td>93</td>\n",
       "      <td>46_agree_agreed_100_totally</td>\n",
       "      <td>[agree, agreed, 100, totally, wholeheartedly, ...</td>\n",
       "      <td>[I agree, I agree, I agree.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>47</td>\n",
       "      <td>93</td>\n",
       "      <td>47_movie_thug_cheryl_watch</td>\n",
       "      <td>[movie, thug, cheryl, watch, 2023, season, mov...</td>\n",
       "      <td>[The Joy Luck Club (book and movie)\\nThelma an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>48</td>\n",
       "      <td>89</td>\n",
       "      <td>48_boys_school_teachers_teacher</td>\n",
       "      <td>[boys, school, teachers, teacher, girls, schoo...</td>\n",
       "      <td>[Can confirm. I’m a woman and taught at a reli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                         Name  \\\n",
       "0      -1  18004                           -1_to_and_they_the   \n",
       "1       0    472           0_4b_movement_feminism_decentering   \n",
       "2       1    358                1_korean_korea_south_movement   \n",
       "3       2    334                            2_dad_she_mom_her   \n",
       "4       3    292                        3_4b_movement_post_6b   \n",
       "5       4    290              4_nan_macaron_joseph_disastrous   \n",
       "6       5    252       5_christmas_holidays_thanksgiving_cook   \n",
       "7       6    246                           6_hugs_tk_oo_vcels   \n",
       "8       7    243          7_trans_cis_transphobia_transphobic   \n",
       "9       8    203          8_marriage_divorce_married_arranged   \n",
       "10      9    192        9_religion_religions_church_religious   \n",
       "11     10    191                 10_song_songs_playlist_music   \n",
       "12     11    190     11_friends_friendships_friend_friendship   \n",
       "13     12    183                  12_proud_thank_hugs_sending   \n",
       "14     13    174                     13_makeup_hair_wear_skin   \n",
       "15     14    164                 14_rape_rapists_rapist_raped   \n",
       "16     15    162   15_liberal_conservative_maga_conservatives   \n",
       "17     16    162                 16_hair_shaving_shave_shaved   \n",
       "18     17    161  17_misandry_misogyny_misandrist_misogynists   \n",
       "19     18    153                         18_join_id_please_hi   \n",
       "20     19    153                   19_ally_allies_allyship_an   \n",
       "21     20    145           20_loneliness_epidemic_lonely_male   \n",
       "22     21    143            21_porn_bdsm_watching_pornography   \n",
       "23     22    133                    22_sub_subs_reddit_banned   \n",
       "24     23    130              23_gay_straight_lesbians_allies   \n",
       "25     24    129                        24_love_this_omg_lmao   \n",
       "26     25    128                 25_fat_body_emmanuel_breasts   \n",
       "27     26    123                         26_dming_you_yup_yes   \n",
       "28     27    120                  27_apps_dating_app_deleting   \n",
       "29     28    120              28_ignore_attention_them_energy   \n",
       "30     29    119                  29_spray_pepper_carry_knife   \n",
       "31     30    114                  30_smile_stare_staring_face   \n",
       "32     31    113               31_men_all_nutshell_bootstraps   \n",
       "33     32    113              32_him_reconcile_he_explanation   \n",
       "34     33    112                  33_gun_guns_defense_handgun   \n",
       "35     34    112          34_custody_motherhood_children_kids   \n",
       "36     35    110   35_therapist_therapists_therapy_statistics   \n",
       "37     36    108                  36_white_black_color_racism   \n",
       "38     37    106   37_movement_separatism_support_separatists   \n",
       "39     38    105                       38_cat_cats_lady_feral   \n",
       "40     39    105                   39_ladies_thank_love_women   \n",
       "41     40    103                       40_they_stupid_us_care   \n",
       "42     41    102                41_dress_wear_dressing_makeup   \n",
       "43     42     98                       42_he_store_smoke_uber   \n",
       "44     43     98            43_violence_violent_crime_victims   \n",
       "45     44     97                  44_gym_weights_gyms_lifting   \n",
       "46     45     96                   45_state_blue_texas_moving   \n",
       "47     46     93                  46_agree_agreed_100_totally   \n",
       "48     47     93                   47_movie_thug_cheryl_watch   \n",
       "49     48     89              48_boys_school_teachers_teacher   \n",
       "\n",
       "                                       Representation  \\\n",
       "0    [to, and, they, the, of, that, it, in, you, for]   \n",
       "1   [4b, movement, feminism, decentering, men, abo...   \n",
       "2   [korean, korea, south, movement, 4b, feminists...   \n",
       "3   [dad, she, mom, her, mother, father, my, he, h...   \n",
       "4   [4b, movement, post, 6b, 7b, relevant, join, i...   \n",
       "5   [nan, macaron, joseph, disastrous, himself, te...   \n",
       "6   [christmas, holidays, thanksgiving, cook, cook...   \n",
       "7   [hugs, tk, oo, vcels, mah, darwin, omega, exho...   \n",
       "8   [trans, cis, transphobia, transphobic, terf, t...   \n",
       "9   [marriage, divorce, married, arranged, marry, ...   \n",
       "10  [religion, religions, church, religious, god, ...   \n",
       "11  [song, songs, playlist, music, listen, album, ...   \n",
       "12  [friends, friendships, friend, friendship, pla...   \n",
       "13  [proud, thank, hugs, sending, strong, you, hop...   \n",
       "14  [makeup, hair, wear, skin, beauty, skincare, w...   \n",
       "15  [rape, rapists, rapist, raped, rapes, marital,...   \n",
       "16  [liberal, conservative, maga, conservatives, w...   \n",
       "17  [hair, shaving, shave, shaved, head, heads, le...   \n",
       "18  [misandry, misogyny, misandrist, misogynists, ...   \n",
       "19  [join, id, please, hi, joined, sign, love, joi...   \n",
       "20  [ally, allies, allyship, an, 4b, here, partici...   \n",
       "21  [loneliness, epidemic, lonely, male, men, them...   \n",
       "22  [porn, bdsm, watching, pornography, consent, s...   \n",
       "23  [sub, subs, reddit, banned, subreddit, mods, s...   \n",
       "24  [gay, straight, lesbians, allies, homophobic, ...   \n",
       "25  [love, this, omg, lmao, brilliant, mindfck, ba...   \n",
       "26  [fat, body, emmanuel, breasts, breast, height,...   \n",
       "27  [dming, you, yup, yes, course, both, sure, me,...   \n",
       "28  [apps, dating, app, deleting, profile, delete,...   \n",
       "29  [ignore, attention, them, energy, engage, wast...   \n",
       "30  [spray, pepper, carry, knife, weapon, carrying...   \n",
       "31  [smile, stare, staring, face, eye, smiling, lo...   \n",
       "32  [men, all, nutshell, bootstraps, pedos, yes, n...   \n",
       "33  [him, reconcile, he, explanation, leave, you, ...   \n",
       "34  [gun, guns, defense, handgun, firearm, carry, ...   \n",
       "35  [custody, motherhood, children, kids, mothers,...   \n",
       "36  [therapist, therapists, therapy, statistics, i...   \n",
       "37  [white, black, color, racism, racist, indigeno...   \n",
       "38  [movement, separatism, support, separatists, f...   \n",
       "39  [cat, cats, lady, feral, dogs, black, kitties,...   \n",
       "40  [ladies, thank, love, women, beautiful, wink, ...   \n",
       "41  [they, stupid, us, care, dont, were, deserve, ...   \n",
       "42  [dress, wear, dressing, makeup, wearing, style...   \n",
       "43  [he, store, smoke, uber, report, bag, shop, hi...   \n",
       "44  [violence, violent, crime, victims, perpetrato...   \n",
       "45  [gym, weights, gyms, lifting, membership, lift...   \n",
       "46  [state, blue, texas, moving, move, florida, lo...   \n",
       "47  [agree, agreed, 100, totally, wholeheartedly, ...   \n",
       "48  [movie, thug, cheryl, watch, 2023, season, mov...   \n",
       "49  [boys, school, teachers, teacher, girls, schoo...   \n",
       "\n",
       "                                  Representative_Docs  \n",
       "0   [> You can ask a man nicely to do something an...  \n",
       "1   [Sorry, but 4b isn't a sort of revenge to make...  \n",
       "2   [and they say there is no 4b movement in Korea...  \n",
       "3   [My mom died of cancer in August this year. I ...  \n",
       "4                         [4B for life! 🙌❤️, 4B, 4B!]  \n",
       "5                                     [nan, nan, nan]  \n",
       "6   [Nope.  I'm spending a quiet Christmas by myse...  \n",
       "7                               [Hugs💜, Hugs💜, Hugs💜]  \n",
       "8   [Hot take, but I'd leave out trans men.\\n\\nTra...  \n",
       "9   [I HATE happy marriage. What now?, Did she say...  \n",
       "10  [Women should start their own religion, free o...  \n",
       "11  [Maybe Studio Killers. I haven't listened much...  \n",
       "12  [I lost so many male friends because they were...  \n",
       "13  [Proud of you., I'm so proud of you!!!, Thank ...  \n",
       "14  [As someone who enjoys makeup, I don’t care ho...  \n",
       "15  [>  A woman supporting rapists is the same as ...  \n",
       "16  [>Oh but I thought maga men DIDN’T want to dat...  \n",
       "17  [Shaving ones head does not make them ugly., I...  \n",
       "18  [Misandry doesn't exist, \"misandry\" lol, So mi...  \n",
       "19  [I’d like to join!, I’d like to join, I’d like...  \n",
       "20  [It only shows why they are really \"allies\", n...  \n",
       "21  [“Male loneliness” They do it to themselves, “...  \n",
       "22  [nope. all porn is rape, all porn is misogynis...  \n",
       "23  [No, it does not need to be shared in women’s ...  \n",
       "24  [lol. Obviously. Recentering men. For example,...  \n",
       "25     [I love this! 🐝🐝🐝🐝, I love this!, I LOVE THIS]  \n",
       "26  [Males fetishize fat. They love fat in breasts...  \n",
       "27               [DMing you!, DMing you!, DMing you!]  \n",
       "28  [Dating apps are for men, and women are the pr...  \n",
       "29  [Ignore. Ignore. Ignore. This is the way., Ign...  \n",
       "30  [Pepper gel is what the cops use.  It's suppos...  \n",
       "31  [Trump has a Grinch smile. They smile exactly ...  \n",
       "32  [All men., All men. , Men have to wear the boo...  \n",
       "33  [Please do not reconcile  with him, he has alr...  \n",
       "34  [Guns, I want a gun like that, Women are more ...  \n",
       "35  [>*\"Abusive men often want and ask for full cu...  \n",
       "36  [No, no, no, no, no. It’s not your job to conv...  \n",
       "37  [She's making points against the Dogma of Whit...  \n",
       "38  [I’m all for 4B women and non-4B women working...  \n",
       "39  [I'm a cat lady! Cats are better companions th...  \n",
       "40  [I love this, thank you so much for the suppor...  \n",
       "41  [Some are stupid enough to actually do that!, ...  \n",
       "42  [Dress for yourself, no-one else, Weird. I dre...  \n",
       "43  [I'm nice to everyone I feel familiar with, es...  \n",
       "44  [I recently came across the Laken Riley murder...  \n",
       "45  [I do private gym or lift weights at home or r...  \n",
       "46  [ I plan to save as much as possible and leave...  \n",
       "47                       [I agree, I agree, I agree.]  \n",
       "48  [The Joy Luck Club (book and movie)\\nThelma an...  \n",
       "49  [Can confirm. I’m a woman and taught at a reli...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info().head(50)\n",
    "\n",
    "# Topics of potential relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1294976-195e-4202-a113-c730d599ea5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[49, 293, 69, 436, 419, 443, 163, 208, 355, 100]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_topics, similarity = topic_model.find_topics(\"abortion\", top_n=10); similar_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ec55467-c857-4326-abb0-cc00ba808449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prolife', 0.05570381860933754),\n",
       " ('profamily', 0.05389564106865588),\n",
       " ('nutrition', 0.043165432415572254),\n",
       " ('pro', 0.04153836244622763),\n",
       " ('born', 0.041380523678211725),\n",
       " ('fetus', 0.03484536479205275),\n",
       " ('denying', 0.03351337136611297),\n",
       " ('prolifer', 0.02942240223083225),\n",
       " ('mitigates', 0.02942240223083225),\n",
       " ('resourcesslaves', 0.02942240223083225)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at top words for a given topic for search terms\n",
    "topic_model.get_topic(436)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8883de0-a2ac-4ffa-8c69-884dcedbafbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "# nltk.download('punkt_tab')\n",
    "\n",
    "# Preprocess the documents\n",
    "tokenized_docs = []\n",
    "for doc in data2:\n",
    "  sentences = nltk.sent_tokenize(doc)\n",
    "  for sentence in sentences:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tokenized_docs.append(tokens)\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c88f5f27-01ce-4d5e-ac79-abae349077e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('outreach', 0.8726297616958618),\n",
       " ('providers', 0.856718897819519),\n",
       " ('medications', 0.8546890616416931),\n",
       " ('conditions', 0.8514281511306763),\n",
       " ('funding', 0.8509233593940735),\n",
       " ('separatism', 0.8503289818763733),\n",
       " ('korea', 0.8497620224952698),\n",
       " ('division', 0.8492761254310608),\n",
       " ('intervention', 0.8421908617019653),\n",
       " ('attitudes', 0.8421791195869446),\n",
       " ('definitions', 0.8419467806816101),\n",
       " ('deaths', 0.8418899774551392),\n",
       " ('acceptance', 0.8398092985153198),\n",
       " ('Peru', 0.8396855592727661),\n",
       " ('sustainability', 0.8391059041023254),\n",
       " ('russia', 0.8387611508369446),\n",
       " ('inequality', 0.838636040687561),\n",
       " ('excluding', 0.8383613228797913),\n",
       " ('fashion', 0.8381003141403198),\n",
       " ('resentment', 0.8376879096031189)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Now we have a model where each word has a similarity score to other words\n",
    "## we can exploit this to search for new words relevant for our theme.\n",
    "\n",
    "model.wv.most_similar(\"activism\", topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb6744c-fdb3-436b-b650-e69636b18180",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can also see which words are similar to a list of words\n",
    "\n",
    "topic_4b_search_words = ['decenter','decentering','ignore','filter']\n",
    "model.wv.most_similar(topic_4b_search_words, topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5e0dce-1dc1-4e23-b991-006bf2442171",
   "metadata": {},
   "source": [
    "## Full clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "090e814e-2f06-4a64-ba90-b72dc8fa8886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bertopic import BERTopic\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.matutils import corpus2csc\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f34b917b-508a-429b-aab3-f3ac9a5d148a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>user code</th>\n",
       "      <th>post code</th>\n",
       "      <th>parent id</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>time_posted</th>\n",
       "      <th>discussion_type</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>downvotes</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>mod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>covrtni</td>\n",
       "      <td>t2_i1uhmwvw</td>\n",
       "      <td>t1_ky8i5zm</td>\n",
       "      <td>t3_1bs8swh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I cant be truly 4b because im in a relationshi...</td>\n",
       "      <td>1.712354e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>comment</td>\n",
       "      <td>/r/4bmovement/comments/1bs8swh/weekly_discussi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unable_Pineapple9211</td>\n",
       "      <td>t2_b10n6x0rk</td>\n",
       "      <td>t1_kyts0zn</td>\n",
       "      <td>t3_1bs8swh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm a lesbian.</td>\n",
       "      <td>1.712697e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>comment</td>\n",
       "      <td>/r/4bmovement/comments/1bs8swh/weekly_discussi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dahlia_74</td>\n",
       "      <td>t2_pblwlerm7</td>\n",
       "      <td>t1_kyydu93</td>\n",
       "      <td>t3_1bwr4cu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We need more people like you ❤️</td>\n",
       "      <td>1.712772e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>comment</td>\n",
       "      <td>/r/4bmovement/comments/1bwr4cu/from_now_on_my_...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dahlia_74</td>\n",
       "      <td>t2_pblwlerm7</td>\n",
       "      <td>t1_kyyemc7</td>\n",
       "      <td>t3_1bs8swh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I just ended things with a FWB who didn’t valu...</td>\n",
       "      <td>1.712773e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>comment</td>\n",
       "      <td>/r/4bmovement/comments/1bs8swh/weekly_discussi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>readditredditread</td>\n",
       "      <td>t2_1rh4e0ns</td>\n",
       "      <td>t1_kyzs2s9</td>\n",
       "      <td>t3_1bkrqs8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>🤏</td>\n",
       "      <td>1.712789e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>comment</td>\n",
       "      <td>/r/4bmovement/comments/1bkrqs8/the_4b_movement...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               username     user code   post code   parent id title  \\\n",
       "0               covrtni   t2_i1uhmwvw  t1_ky8i5zm  t3_1bs8swh   NaN   \n",
       "1  Unable_Pineapple9211  t2_b10n6x0rk  t1_kyts0zn  t3_1bs8swh   NaN   \n",
       "2             dahlia_74  t2_pblwlerm7  t1_kyydu93  t3_1bwr4cu   NaN   \n",
       "3             dahlia_74  t2_pblwlerm7  t1_kyyemc7  t3_1bs8swh   NaN   \n",
       "4     readditredditread   t2_1rh4e0ns  t1_kyzs2s9  t3_1bkrqs8   NaN   \n",
       "\n",
       "                                             content   time_posted  \\\n",
       "0  I cant be truly 4b because im in a relationshi...  1.712354e+09   \n",
       "1                                     I'm a lesbian.  1.712697e+09   \n",
       "2                    We need more people like you ❤️  1.712772e+09   \n",
       "3  I just ended things with a FWB who didn’t valu...  1.712773e+09   \n",
       "4                                                  🤏  1.712789e+09   \n",
       "\n",
       "  discussion_type  upvotes  downvotes     type  \\\n",
       "0             NaN        2          0  comment   \n",
       "1             NaN        2          0  comment   \n",
       "2             NaN        1          0  comment   \n",
       "3             NaN        5          0  comment   \n",
       "4             NaN        1          0  comment   \n",
       "\n",
       "                                                 url    mod  \n",
       "0  /r/4bmovement/comments/1bs8swh/weekly_discussi...  False  \n",
       "1  /r/4bmovement/comments/1bs8swh/weekly_discussi...  False  \n",
       "2  /r/4bmovement/comments/1bwr4cu/from_now_on_my_...  False  \n",
       "3  /r/4bmovement/comments/1bs8swh/weekly_discussi...  False  \n",
       "4  /r/4bmovement/comments/1bkrqs8/the_4b_movement...  False  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('merged.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae21bedf-8e60-478c-8523-3c2200c833d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39magg(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Step 2: Clean text\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Step 3: Drop empty/invalid rows\u001b[39;00m\n\u001b[0;32m     41\u001b[0m cleaned_df \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sds\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4919\u001b[0m         func,\n\u001b[0;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sds\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sds\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sds\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sds\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[7], line 29\u001b[0m, in \u001b[0;36mpreprocess\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     23\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text\u001b[38;5;241m.\u001b[39mlower())\n\u001b[0;32m     24\u001b[0m pos_tags \u001b[38;5;241m=\u001b[39m pos_tag(tokens)\n\u001b[0;32m     26\u001b[0m lemmatized \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     27\u001b[0m     lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word, get_wordnet_pos(pos))\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word, pos \u001b[38;5;129;01min\u001b[39;00m pos_tags\n\u001b[1;32m---> 29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39misalpha() \u001b[38;5;129;01mand\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words\n\u001b[0;32m     30\u001b[0m ]\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lemmatized) \u001b[38;5;28;01mif\u001b[39;00m lemmatized \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stop_words' is not defined"
     ]
    }
   ],
   "source": [
    "# POS tag to WordNet tag\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Text cleaning and preprocessing\n",
    "def preprocess(text):\n",
    "    if pd.isnull(text) or text.strip().lower() in ['removed', 'eliminated', '']:\n",
    "        return np.nan\n",
    "\n",
    "    text = re.sub(r'@\\w+', '', text)         # Remove mentions\n",
    "    text = re.sub(r'\\d+', '', text)          # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Normalize whitespace\n",
    "\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    lemmatized = [\n",
    "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "        for word, pos in pos_tags\n",
    "        if word.isalpha() and word not in stop_words\n",
    "    ]\n",
    "\n",
    "    return ' '.join(lemmatized) if lemmatized else np.nan\n",
    "\n",
    "# Step 1: Combine 'title' and 'content'\n",
    "df['combined'] = df[['title', 'content']].fillna('').agg(' '.join, axis=1)\n",
    "\n",
    "# Step 2: Clean text\n",
    "df['cleaned'] = df['combined'].apply(preprocess)\n",
    "\n",
    "# Step 3: Drop empty/invalid rows\n",
    "cleaned_df = df['cleaned'].dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc362ea4-2046-4754-bfe7-940d3b138fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 21:54:50,573 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6600d7b4ecbf43cfbe83d1c63a0eeaa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1346 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 22:02:27,682 - BERTopic - Embedding - Completed ✓\n",
      "2025-05-19 22:02:27,698 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-05-19 22:04:07,727 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-05-19 22:04:07,751 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-05-19 22:40:42,821 - BERTopic - Cluster - Completed ✓\n",
      "2025-05-19 22:40:43,464 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-05-19 22:40:46,976 - BERTopic - Representation - Completed ✓\n"
     ]
    }
   ],
   "source": [
    "# Fit BERTopic\n",
    "topic_model = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True)\n",
    "topics, probs = topic_model.fit_transform(cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7eba873-79b6-4c6d-aa68-7e853f4e8900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>19170</td>\n",
       "      <td>-1_woman_men_life_want</td>\n",
       "      <td>[woman, men, life, want, like, get, make, thin...</td>\n",
       "      <td>[year old forb movement year without realize t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3281</td>\n",
       "      <td>0_remove_awesomeeee_reductionary_tired</td>\n",
       "      <td>[remove, awesomeeee, reductionary, tired, tmi,...</td>\n",
       "      <td>[remove, remove, remove]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2369</td>\n",
       "      <td>1_remove_hahahahhahahah_hannam_pleeease</td>\n",
       "      <td>[remove, hahahahhahahah, hannam, pleeease, orn...</td>\n",
       "      <td>[remove, remove, remove]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>784</td>\n",
       "      <td>2_vote_white_trump_conservative</td>\n",
       "      <td>[vote, white, trump, conservative, liberal, el...</td>\n",
       "      <td>[dont vote man election country, yeah ya part ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>383</td>\n",
       "      <td>3_abortion_pregnancy_pregnant_baby</td>\n",
       "      <td>[abortion, pregnancy, pregnant, baby, birth, b...</td>\n",
       "      <td>[abortion ban good option, abortion woman preg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>372</td>\n",
       "      <td>4_gun_spray_carry_defense</td>\n",
       "      <td>[gun, spray, carry, defense, pepper, knife, we...</td>\n",
       "      <td>[buy gun learn use carry, woman martial art tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>319</td>\n",
       "      <td>5_rape_rapist_consent_rap</td>\n",
       "      <td>[rape, rapist, consent, rap, assault, crime, v...</td>\n",
       "      <td>[woman support rapist rapist would never say m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>312</td>\n",
       "      <td>6_makeup_dress_wear_hair</td>\n",
       "      <td>[makeup, dress, wear, hair, beauty, clothes, s...</td>\n",
       "      <td>[beauty makeup, go still wear makeup want also...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>304</td>\n",
       "      <td>7_movement_protest_radical_participate</td>\n",
       "      <td>[movement, protest, radical, participate, femi...</td>\n",
       "      <td>[new maybe try read movement say world turn up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>297</td>\n",
       "      <td>8_iud_hysterectomy_uterus_period</td>\n",
       "      <td>[iud, hysterectomy, uterus, period, hormonal, ...</td>\n",
       "      <td>[period almost year iud though awesome otherwi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>275</td>\n",
       "      <td>9_song_playlist_music_album</td>\n",
       "      <td>[song, playlist, music, album, listen, paris, ...</td>\n",
       "      <td>[b music recommendation hi everyone felt like ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>263</td>\n",
       "      <td>10_marry_divorce_marriage_wife</td>\n",
       "      <td>[marry, divorce, marriage, wife, married, husb...</td>\n",
       "      <td>[divorce, still marry, never marry never go ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>262</td>\n",
       "      <td>11_korean_korea_south_movement</td>\n",
       "      <td>[korean, korea, south, movement, feminist, chi...</td>\n",
       "      <td>[w south korean sister, korean woman tell remo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>256</td>\n",
       "      <td>12_walk_loud_ignore_store</td>\n",
       "      <td>[walk, loud, ignore, store, move, grocery, doo...</td>\n",
       "      <td>[surprise one bring follow yet american observ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>216</td>\n",
       "      <td>13_christmas_cook_holiday_thanksgiving</td>\n",
       "      <td>[christmas, cook, holiday, thanksgiving, dinne...</td>\n",
       "      <td>[many cook men holiday despite b notice lot wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14</td>\n",
       "      <td>209</td>\n",
       "      <td>14_trans_transphobia_cis_transphobic</td>\n",
       "      <td>[trans, transphobia, cis, transphobic, inclusi...</td>\n",
       "      <td>[talk see even one group talk feminism whole b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15</td>\n",
       "      <td>208</td>\n",
       "      <td>15_religion_catholic_church_religious</td>\n",
       "      <td>[religion, catholic, church, religious, god, w...</td>\n",
       "      <td>[woman start religion free men, religion, b re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16</td>\n",
       "      <td>204</td>\n",
       "      <td>16_friendship_friend_platonic_befriend</td>\n",
       "      <td>[friendship, friend, platonic, befriend, zone,...</td>\n",
       "      <td>[close male friend friendship possible good se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17</td>\n",
       "      <td>150</td>\n",
       "      <td>17_teacher_school_boy_grade</td>\n",
       "      <td>[teacher, school, boy, grade, student, class, ...</td>\n",
       "      <td>[fact word put studious girl next rowdy boys s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18</td>\n",
       "      <td>145</td>\n",
       "      <td>18_porn_pornography_iwf_kink</td>\n",
       "      <td>[porn, pornography, iwf, kink, watch, search, ...</td>\n",
       "      <td>[also porn, porn, porn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19</td>\n",
       "      <td>130</td>\n",
       "      <td>19_shave_hair_head_bald</td>\n",
       "      <td>[shave, hair, head, bald, leg, shaved, alopeci...</td>\n",
       "      <td>[imagine shave head think something remove, sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20</td>\n",
       "      <td>127</td>\n",
       "      <td>20_fart_wash_perfume_smell</td>\n",
       "      <td>[fart, wash, perfume, smell, pad, clean, wipe,...</td>\n",
       "      <td>[cat fart face let literally walk, op aio post...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>21</td>\n",
       "      <td>123</td>\n",
       "      <td>21_dming_vcels_deick_neeeed</td>\n",
       "      <td>[dming, vcels, deick, neeeed, dsgbv, bell, sig...</td>\n",
       "      <td>[dming, dming, dming]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>22</td>\n",
       "      <td>118</td>\n",
       "      <td>22_remove_beware_butterball_slocking</td>\n",
       "      <td>[remove, beware, butterball, slocking, importa...</td>\n",
       "      <td>[beware say remove, go remove, like remove]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>23</td>\n",
       "      <td>116</td>\n",
       "      <td>23_loneliness_epidemic_lonely_male</td>\n",
       "      <td>[loneliness, epidemic, lonely, male, cry, frie...</td>\n",
       "      <td>[male loneliness epidemic men make good partne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>24</td>\n",
       "      <td>115</td>\n",
       "      <td>24_orgasm_clitoris_pleasure_masturbate</td>\n",
       "      <td>[orgasm, clitoris, pleasure, masturbate, orgas...</td>\n",
       "      <td>[imagine man say care orgasm sex, join b many ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>25</td>\n",
       "      <td>115</td>\n",
       "      <td>25_remove_femboys_aggravating_insight</td>\n",
       "      <td>[remove, femboys, aggravating, insight, men, c...</td>\n",
       "      <td>[accident delete live roof mom center men aggr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>26</td>\n",
       "      <td>114</td>\n",
       "      <td>26_smile_stare_eye_compliment</td>\n",
       "      <td>[smile, stare, eye, compliment, contact, gaze,...</td>\n",
       "      <td>[love hear make smile, smile everyone would li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>27</td>\n",
       "      <td>109</td>\n",
       "      <td>27_love_cute_everloving_bahah</td>\n",
       "      <td>[love, cute, everloving, bahah, savage, honest...</td>\n",
       "      <td>[love, love, love]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>28</td>\n",
       "      <td>109</td>\n",
       "      <td>28_gym_weight_lift_membership</td>\n",
       "      <td>[gym, weight, lift, membership, exercise, work...</td>\n",
       "      <td>[stop go gym start lift weight home run outsid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                     Name  \\\n",
       "0      -1  19170                   -1_woman_men_life_want   \n",
       "1       0   3281   0_remove_awesomeeee_reductionary_tired   \n",
       "2       1   2369  1_remove_hahahahhahahah_hannam_pleeease   \n",
       "3       2    784          2_vote_white_trump_conservative   \n",
       "4       3    383       3_abortion_pregnancy_pregnant_baby   \n",
       "5       4    372                4_gun_spray_carry_defense   \n",
       "6       5    319                5_rape_rapist_consent_rap   \n",
       "7       6    312                 6_makeup_dress_wear_hair   \n",
       "8       7    304   7_movement_protest_radical_participate   \n",
       "9       8    297         8_iud_hysterectomy_uterus_period   \n",
       "10      9    275              9_song_playlist_music_album   \n",
       "11     10    263           10_marry_divorce_marriage_wife   \n",
       "12     11    262           11_korean_korea_south_movement   \n",
       "13     12    256                12_walk_loud_ignore_store   \n",
       "14     13    216   13_christmas_cook_holiday_thanksgiving   \n",
       "15     14    209     14_trans_transphobia_cis_transphobic   \n",
       "16     15    208    15_religion_catholic_church_religious   \n",
       "17     16    204   16_friendship_friend_platonic_befriend   \n",
       "18     17    150              17_teacher_school_boy_grade   \n",
       "19     18    145             18_porn_pornography_iwf_kink   \n",
       "20     19    130                  19_shave_hair_head_bald   \n",
       "21     20    127               20_fart_wash_perfume_smell   \n",
       "22     21    123              21_dming_vcels_deick_neeeed   \n",
       "23     22    118     22_remove_beware_butterball_slocking   \n",
       "24     23    116       23_loneliness_epidemic_lonely_male   \n",
       "25     24    115   24_orgasm_clitoris_pleasure_masturbate   \n",
       "26     25    115    25_remove_femboys_aggravating_insight   \n",
       "27     26    114            26_smile_stare_eye_compliment   \n",
       "28     27    109            27_love_cute_everloving_bahah   \n",
       "29     28    109            28_gym_weight_lift_membership   \n",
       "\n",
       "                                       Representation  \\\n",
       "0   [woman, men, life, want, like, get, make, thin...   \n",
       "1   [remove, awesomeeee, reductionary, tired, tmi,...   \n",
       "2   [remove, hahahahhahahah, hannam, pleeease, orn...   \n",
       "3   [vote, white, trump, conservative, liberal, el...   \n",
       "4   [abortion, pregnancy, pregnant, baby, birth, b...   \n",
       "5   [gun, spray, carry, defense, pepper, knife, we...   \n",
       "6   [rape, rapist, consent, rap, assault, crime, v...   \n",
       "7   [makeup, dress, wear, hair, beauty, clothes, s...   \n",
       "8   [movement, protest, radical, participate, femi...   \n",
       "9   [iud, hysterectomy, uterus, period, hormonal, ...   \n",
       "10  [song, playlist, music, album, listen, paris, ...   \n",
       "11  [marry, divorce, marriage, wife, married, husb...   \n",
       "12  [korean, korea, south, movement, feminist, chi...   \n",
       "13  [walk, loud, ignore, store, move, grocery, doo...   \n",
       "14  [christmas, cook, holiday, thanksgiving, dinne...   \n",
       "15  [trans, transphobia, cis, transphobic, inclusi...   \n",
       "16  [religion, catholic, church, religious, god, w...   \n",
       "17  [friendship, friend, platonic, befriend, zone,...   \n",
       "18  [teacher, school, boy, grade, student, class, ...   \n",
       "19  [porn, pornography, iwf, kink, watch, search, ...   \n",
       "20  [shave, hair, head, bald, leg, shaved, alopeci...   \n",
       "21  [fart, wash, perfume, smell, pad, clean, wipe,...   \n",
       "22  [dming, vcels, deick, neeeed, dsgbv, bell, sig...   \n",
       "23  [remove, beware, butterball, slocking, importa...   \n",
       "24  [loneliness, epidemic, lonely, male, cry, frie...   \n",
       "25  [orgasm, clitoris, pleasure, masturbate, orgas...   \n",
       "26  [remove, femboys, aggravating, insight, men, c...   \n",
       "27  [smile, stare, eye, compliment, contact, gaze,...   \n",
       "28  [love, cute, everloving, bahah, savage, honest...   \n",
       "29  [gym, weight, lift, membership, exercise, work...   \n",
       "\n",
       "                                  Representative_Docs  \n",
       "0   [year old forb movement year without realize t...  \n",
       "1                            [remove, remove, remove]  \n",
       "2                            [remove, remove, remove]  \n",
       "3   [dont vote man election country, yeah ya part ...  \n",
       "4   [abortion ban good option, abortion woman preg...  \n",
       "5   [buy gun learn use carry, woman martial art tr...  \n",
       "6   [woman support rapist rapist would never say m...  \n",
       "7   [beauty makeup, go still wear makeup want also...  \n",
       "8   [new maybe try read movement say world turn up...  \n",
       "9   [period almost year iud though awesome otherwi...  \n",
       "10  [b music recommendation hi everyone felt like ...  \n",
       "11  [divorce, still marry, never marry never go ma...  \n",
       "12  [w south korean sister, korean woman tell remo...  \n",
       "13  [surprise one bring follow yet american observ...  \n",
       "14  [many cook men holiday despite b notice lot wo...  \n",
       "15  [talk see even one group talk feminism whole b...  \n",
       "16  [woman start religion free men, religion, b re...  \n",
       "17  [close male friend friendship possible good se...  \n",
       "18  [fact word put studious girl next rowdy boys s...  \n",
       "19                            [also porn, porn, porn]  \n",
       "20  [imagine shave head think something remove, sh...  \n",
       "21  [cat fart face let literally walk, op aio post...  \n",
       "22                              [dming, dming, dming]  \n",
       "23        [beware say remove, go remove, like remove]  \n",
       "24  [male loneliness epidemic men make good partne...  \n",
       "25  [imagine man say care orgasm sex, join b many ...  \n",
       "26  [accident delete live roof mom center men aggr...  \n",
       "27  [love hear make smile, smile everyone would li...  \n",
       "28                                 [love, love, love]  \n",
       "29  [stop go gym start lift weight home run outsid...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Topics of potential relevance\n",
    "topics=topic_model.get_topic_info().head(30)\n",
    "\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b12c6346-b88c-48de-8bda-dd4eadbb0e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics.to_csv('BERT_topics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "367f267f-18c4-44ae-8d06-d28f267265e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topic_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m topic_model\u001b[38;5;241m.\u001b[39mget_topic(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'topic_model' is not defined"
     ]
    }
   ],
   "source": [
    "topic_model.get_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a2a0c1-40fb-4f4b-a083-9fb933508f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
